master thesis
修論スケルトン
■ Abstract
English

■ 目次
自動生成

■ はじめに
スマホの普及 > 個人が所有するモバイル端末の数が増えている > モバイル端末間での連携に注目 > モバイル端末連携の例と問題点
スマホ内蔵のセンサ > NUI(音声やタッチ、ジェスチャ) > センサを用いたNUIによる連携の関連研究 > 関連研究の問題点
既存の問題点を解消する自分のシステム > 先行研究 > システムの概要
論文の構成


■ 複数のモバイル端末連携システム（関連研究）
カメラ、加速度センサ、マイク、特殊なセンサを用いた研究とそれらの問題点
・概要
複数のモバイル端末連携システムとは > メリット
・カメラ
・マイク
・加速度センサ
・特殊なセンサ

（■ 先行研究（本谷さんの研究: 参照するし、なくてもいい？））

■ 異種端末での精度検証実験
・概要
(本谷さんの研究 > )本谷さんの研究では1機種でしか試していなかった > 一方、本システムでは異機種での利用が想定される
他の端末でも利用可能かを検証 >
・内蔵照度センサの性能検証実験
概要 > 結果 > 考察
・異機種におけるジェスチャ認識精度検証実験
概要 > 結果 > 考察

■ 提案システム（IllumiConnect（仮））
・概要
システムについて > メリット
・ジェスチャの種類
・構成
構成図
フロー
・特徴抽出および分類手法
アルゴリズム

■ 応用例（アプリケーション）
・データ共有

・ディスプレイ合体
・サラウンドシステム
・チーム分け ...
・スマホ操作

■ IllumiConnectの検証実験
・概要
・環境
・to, up, down実験
・結果
・考察
・order実験
・結果
・考察

■ 結論

■ 今後の展望

■ 参考文献

■ 謝辞


--------------------------------------------------------------------------------------------------------------------
ストーリー

最近、みんなスマホ持ってるよね
しかも、一人でスマホとタブレットとスマートウォッチと…って
世の中にスマホ的な端末増えてきてる
ということで、こいつらの間でデータ送信できるようになったり、ディスプレイ合わせて大きいディスプレイにするとかできたらよくない？
しかも、なんかごちゃごちゃむずいことせんとNUIでぱっとできたらうれしくない？
実は、こうゆうのできるようにするために色々研究やられてるねん
最近のスマホはいろんなセンサ入ってるから、加速度センサとかカメラとかマイクとか、そういうの使ってシステム組んでてるんよ
でも、こうゆうセンサってやっぱ状況によってはなかなか使えない場面あるよね
ということで、照度センサでもこうゆうのつくってみようと思ったわけですよ

僕が作ったシステムでは、端末の上で手を動かすと、ぱっとデータ送信できたり、どこに端末あるかわかるようになったりすんのね
どうやってるかっていうと
スマホからサーバにデータ送って処理して、返す


まずは、どのジェスチャかを分類するために、正しいデータを用意します。
そのデータから特徴的な部分を引っ張り出してきます。
そして、その特徴を基にして、分類器を作ります。
これで準備はできたので、適当にジェスチャをすると、この分類器に入って、答えが出てくるという寸法です。
さらに詳しいことをいうと、
各端末は照度センサで取得した値を0.2sに一回くらいのペースでサーバにぶっとばします。
サーバはその値からジェスチャの開始点と終了点を見つけ出して、切り出し、特徴的な値をばーと取ってきます。
それを、各端末ごとにやると、各端末ごとの特徴量出てきます。
さらに、端末毎のその特徴を比較して、特徴を抽出し、こいつを特徴とます。
ユーザが使うときは、分類器が既にできてるはずなので、抽出した特徴量を、分類器に入れると、勝手に探索して答えをくれます。
この答えを、サーバから各端末におくります。
各端末は、その答えに合わせて動作することで、データ送信なり、ディスプレイ共有なり、なんなりできるようになるというわけです。

今回のこのシステムが、ちゃんとしてるのかを検証したかったから2つ実験しました。
1つ目は、ジェスチャの認識精度検証です。
ふつうにやるパターン、先にやったやつが次のやつにいったときにまだ影になってるパターン、
逆に先にやろうとしたときにすでに、次やろうとしてるやつの影になってるパターン、の3つのパターンで
水平、斜め上、斜め下の3つのジェスチャを認識できるか、9パターンを検証する
9＊10＊ｎ
実験の結果高い認識精度で分類可能なことがわかりました。
（特徴量をどうするか、分けて考えてもいいかも）

2つ目は、順番通りに認識できるかを検証する。
正しい順番で認識できるかを検証する
ジェスチャは水平のみで、4デバイスが6通り、3デバイスが6通り，2デバイスが3通り合計15通り*始点4つだから60通り
60＊2＊n
実験の結果、順番はほとんど間違うことがなかった、このことから、デバイスの数が増えても対応できるはずよ

こいつの実用例としてはこういうことに使えるんじゃないかと考えてます。

ただ、まだ、こーゆうとこがだめなんでこの辺は引き継いでいってくれると嬉しいよ！

以上です。




--------------------------------------------------------------------------------------------------------------------
Memo
■ 本谷さんの修論に書いてたこと
・関連研究
SideShape, Surface Link, In air gesture
機種依存: 三木研 kuwazima Ikegami Azuma An Intelligent Lighting System using a SmartPhone as An Illuminance Sensor

Conductor: Enabling and Understanding Cross-Device Interaction

■　非接触型モーションセンサによるジェスチャ認識の研究
Leep Motionなどのデバイスでは，Web Socketでデータを送信し，Webブラウザ上で処理し表現することもできる．
だがWebブラウザ上で機械学習といった手法を適用するのは難しいため，Webブラウザ上などでも認識されるよう，なるべく軽量かつ，オフラインで完結するような実装が必要であると考えられる．
Webサーバ上で処理させていては通信の時間がかかってしまい，リアルタイム性が求められるジェスチャ認識には向かないと考えられるからである．

■気づき
・4台以上のときの問題
★始点を決める most important
★一度通った端末には戻らない important
小さすぎる山は除外する
返答までの時間は？
縦方向は言い方カエル
bluetoothとかでやるべき？
おふらいん
スマホだけでできる
サーバいんの説
なくてもできると思うけど、今回の研究の目的は...
まあできるやろ感はどうするか
4台でやると被験者実験の一人当たりの数がやばくなる説
 7月 1800 = 300(5gestures * 6devices * 10times) * 6subjects
 本谷さん 1400 = 200(5gestures * 4environment * 10times) * 7subjects
 360 (3gestures(to, up, down) * 6 passes * 2 ToAndFrom *10)
 ↓
 もしかしたら
 90 (3gestures * 3 pattern(horizontal(right and left), top, bottom) * 10times)
 でもいいかも
